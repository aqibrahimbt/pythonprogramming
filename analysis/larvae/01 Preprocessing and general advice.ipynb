{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to approach reading in real-world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you work with real-world data the data is usually not ready for analysis directly. We have to do some preprocessing first. That means that you write a program that takes the original (bad) data and then this program creates a new dataset (usually split across several files). And this new dataset is what you will analyse.\n",
    "\n",
    "In general it is a good idea to not solve all problems at the same time. That it is why you first want to produce clean and well-structured data and then do the actual analysis later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing clean datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what do we need to produce a clean dataset? There are several aspects to it.\n",
    "\n",
    "First of all, you want to have regular data values. If there are some misspellings or invalid values in the original data, you want to correct these things automatically when you produce clean data from the original data. This is really a matter of writing *individual rules* about details. This can be very annoying because you basically have to cover all the bad details in the dataset with your program. But once you have written the program you have clean data! And if you later discover that there are still some invalid values, you extend the program, run it again and then keep working with the clean data. Furthermore, by writing a program that creates a new version of the data, you have documented your changes and corrections in a reliable way. If you change data manually you will probably not remember what you have changed a couple of weeks later.\n",
    "\n",
    "Secondly, you want to split your data so that each dataset has its own file. For example, if you have an Excel sheet with measurements and further parameters that describe the experiment itself, split this information into two files: One file with the measurements and another file with the additional parameters. This way you can easily read the information that you want to read without jumping around in the file with your program. (This point is less relevant for the drosophila data but applies even more so to the data from the redox experiments).\n",
    "\n",
    "Thirdly, you probably need to restructure the data. There is a convention that is becoming more and more widespread in the recent years: *Tidy data*. Tidy data follows the following rules:\n",
    "\n",
    "* Each variable forms a column\n",
    "* Each observation forms a row\n",
    "* Each type of observation forms a table\n",
    "\n",
    "For example, the following structure is a tidy dataset:\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr><th>name</th><th>treatment</th><th>treatment_result</th></tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>John Smith</td><td>A</td><td></td></tr>\n",
    "<tr><td>Jane Doe</td><td>A</td><td>16</td></tr>\n",
    "<tr><td>Mary Johnson</td><td>A</td><td>3</td></tr>\n",
    "<tr><td>John Smith</td><td>B</td><td>2</td></tr>\n",
    "<tr><td>Jane Doe</td><td>B</td><td>11</td></tr>\n",
    "<tr><td>Mary Johnson</td><td>B</td><td>1</td></tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "(Table adapted from <a href=\"http://vita.had.co.nz/papers/tidy-data.pdf\" target=\"_blank\">Hadley Wickham's tidy data paper</a>)\n",
    "\n",
    "Note that the names occur in several rows. That is because they are associated with different *observations*. Each column is a variable. The variables are `name`, `treatment` and `treatment_result`. Data that is structured in such a way is much easier to process with a program. Also note that missing values are represented by an empty cell. If you have a dataset where all missing values have the same representation it is easier to write a program that knows how to recognise such missing values.\n",
    "\n",
    "Note that the things above talk about how you store data in files. Once you have the data loaded into your Python program you can create other structures that are more convenient for processing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaching the Larva data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, the dataset is pretty clean. So let us think about what kind of things there are in the data set:\n",
    "\n",
    "* name/index of the larva\n",
    "* time\n",
    "* acc_dst\n",
    "* acceleration\n",
    "* area\n",
    "* bending\n",
    "* dst_to_origin\n",
    "* go_phase\n",
    "* head_x\n",
    "* head_y\n",
    "* is_coiled\n",
    "* is_well_oriented\n",
    "* left_bended\n",
    "* mom_dst\n",
    "* mom_x\n",
    "* mom_y\n",
    "* mov_direction\n",
    "* perimeter\n",
    "* radius_1\n",
    "* right_bended\n",
    "* spine_length\n",
    "* spinepoint_1_x\n",
    "* spinepoint_1_y\n",
    "* tail_x\n",
    "* tail_y\n",
    "* velocity\n",
    "\n",
    "My suggestion:\n",
    "Each observation is dependent on the time. The variables are the measurements. Each larva should get its own table. So that we will have the following structure:\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr><th>time</th><th>acc_dst</th><th>acceleration</th><th>...</th><th>velocity</th></tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>0</td><td>3</td><td>-0,208191</td><td>...</td><td>0,608276</td></tr>\n",
    "<tr><td>1</td><td>3</td><td>-0,840494</td><td>...</td><td>0,894284</td></tr>\n",
    "<tr><td>2</td><td>3</td><td>-0,949482</td><td>...</td><td>0,289482</td></tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "In general: When you write a program you should ask yourself if it will run just a few times or frequently. If it runs frequently you should optimise it for speed. But data cleaning programs usually only run a few times and they tend to be fast enough in practise. So it is okay to write your programs in an easier way even if it means that the program is less efficient. When you write your program in an easier way it is also much less likely that you make mistakes.\n",
    "\n",
    "What does that mean in practise? In this case we will open each file several times, because there are several larvae measurements in each file. In principle it is possible to write a program that collects all larvae measurements from a file at once but that makes the program more complex and also more error-prone (and also more difficult for me to explain to your)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# For this code we will use a defaultdict. A defaultdict is a dictionary that behaves\n",
    "# exactly like a normal dictionary but if you ask for a key that is not in the dictionary\n",
    "# the defaultdict will automatically create a new value for the missing key and from\n",
    "# the outside it will not be apparent if the value is new or old. This is very handy\n",
    "# because it means that you do not have to provide initial values for the dictionary\n",
    "# contents. A defaultdict can be used like this:\n",
    "#\n",
    "# `my_dict = defaultdict( list )`\n",
    "#\n",
    "# And when a missing key is requested the defaultdict will execute\n",
    "#\n",
    "# `new_value = list()`\n",
    "#\n",
    "# and the new value will be saved in the dictionary and returned\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def clean_larva_data_from_file( filename, larva_index, outfile ):\n",
    "    \"\"\"\n",
    "    Functions can be documented by adding a multi-line string after the `def` line.\n",
    "    You can look up this documentation by running `help(read_larva_data_from_file)`\n",
    "    in Jupyter or in a Python console.\n",
    "    \n",
    "    Multiline-strings are defined by three quote characters in the beginning and\n",
    "    the same three quote characters at the end of the string.\n",
    "    \n",
    "    There are some conventions on how to document functions (and methods).\n",
    "    For example, the Sphinx convention asks you to document your parameters\n",
    "    by using the :param directive followed by the name of the parameter plus\n",
    "    a colon and the description of the parameter:\n",
    "    \n",
    "    :param filename: The path to the Drosophila larvae measurements file\n",
    "    :param larva_index: The column index of the drosophila larva\n",
    "    :param outfile: The path of the clean output file\n",
    "    \"\"\" #this is the end of the multi-line string\n",
    "    \n",
    "    #structured_data: { variable -> [ measurements_at_different_time_points ] } #It is always a good idea to sketch out the structure of your data\n",
    "    structured_data = defaultdict( list ) # defaultdict: a dictionary that will insert a default value if we use a new key. In this case the default key will be an empty list\n",
    "    \n",
    "    #first part of this function: Read in data and store it in the variable structured_data\n",
    "    \n",
    "    with open( filename ) as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip( '\\n' ).split( '\\t' ) # parts are the individual cells in a row\n",
    "            \n",
    "            # I use a cheap trick to separate the label from the time. I basically split the string\n",
    "            # at the `(` parenthesis. The stuff that comes before the `(` is the label and the stuff\n",
    "            # that comes after the `(` is the time plus the closing parenthesis. I get rid of the\n",
    "            # closing parenthesis by saying [ :-1 ] which takes a substring without the last letter.\n",
    "            label = parts[ 0 ].split( '(' )[ 0 ] # from \"label(time)\" extract \"label\"\n",
    "            if label == '': #if we are in the header row\n",
    "                continue # jump back to line 16 and read the next row\n",
    "            time = parts[ 0 ].split( '(' )[ 1 ][ :-1 ] #from \"label(time)\" extract \"time\"\n",
    "            value = parts[ larva_index ]\n",
    "\n",
    "            \n",
    "            #assert is a Python mechanism for sanity checks.\n",
    "            #It will throw a big error if the following line is not True\n",
    "            #We assume that there is a measurement for each variable at each time. And that time is increasing by 1 between measurements.\n",
    "            #In general you use asserts to document your assumptions about what the program assumes to be true.\n",
    "            #If an assumptions is false, the `assert` will complain and you will know that need to reconsider your assumptions.\n",
    "            assert len( structured_data[ label ] ) == int( time ), \"Time is not increasing in steps of 1\" \n",
    "            \n",
    "            structured_data[ label ].append( value ) # this is where the defaultdict comes in handy. If the label key would be absent we would get an error. (same applies to the line with the `assert`)\n",
    "    \n",
    "    #second part of this function: Write the data in structured_data to the output file\n",
    "    \n",
    "    with open( outfile, \"w\" ) as out:\n",
    "        labels = structured_data.keys() # aka column headers or variables\n",
    "        \n",
    "        #write header line\n",
    "        out.write( 'time\\t{}\\n'.format( '\\t'.join( labels ) ) )\n",
    "        \n",
    "        # each row corresponds to a time point\n",
    "        for time in range( len( structured_data[ label ] ) ): #it does not matter which label we use as long as it is a valid label\n",
    "\n",
    "            # each column corresponds to a label/variable\n",
    "            values = [] # temporary storage for the values for the current row\n",
    "            for label in labels:\n",
    "                values.append( structured_data[ label ][ time ] ) #gather all values for the current time\n",
    "\n",
    "            out.write( \"{}\\t{}\\n\".format( time, '\\t'.join( values ) ) ) #write a tidy row\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Call the abov function with one dataset for the first larva\n",
    "clean_larva_data_from_file( 'measurements/Results_video1.csv', 1, \"measurements/Results_video1_larva_col1.csv\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And the results are really nice! Check them out <a href=\"measurements/Results_video1_larva_col1.csv\" download=\"Results_video1_larva_col1.csv\">here</a>. If you open the document in MS Excel or OpenOffice/LibreOffice, make sure you select \"tab\" as the column separator. If you get asked about the file encoding, choose UTF-8.\n",
    "\n",
    "But wait! That is only one single dataset! Let's just ask the computer to convert them all for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We have to do a bit of planning, because not all spreadsheets have the same number of larvae in them\n",
    "number_of_larvae = {\n",
    "    1: 2, # two larvae in Results_video1.csv file (we already processed this one in the previous cell)\n",
    "    2: 4,\n",
    "    3: 3,\n",
    "    5: 2,\n",
    "    6: 1,\n",
    "    7: 2,\n",
    "    8: 1,\n",
    "    9: 2,\n",
    "}\n",
    "# We could also have written another program that gives us the number of columns for each file.\n",
    "# But well, I just wrote down this dictionary in 2 minutes :)\n",
    "# Keep in mind that in the future, if you should get more files, it is a better idea to automate this part as well.\n",
    "\n",
    "for video_id, larva_count in number_of_larvae.items():\n",
    "    for column_index in range( 1, larva_count + 1 ): #start at 1, end at larva_count\n",
    "        clean_larva_data_from_file(\n",
    "            'measurements/Results_video{}.csv'.format( video_id ), #input file\n",
    "            column_index, #larva located at column `column_index`\n",
    "            'measurements/Results_video{}_larva_col{}.csv'.format( video_id, column_index ) #output file\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that took less than a second! Let's take a look at the generated files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "sorted( listdir( 'measurements/' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from here on we will proceed with the analysis! I recommend to create a fresh notebook for this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
